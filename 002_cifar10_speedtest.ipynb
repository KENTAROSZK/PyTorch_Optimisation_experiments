{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cca3fbe",
   "metadata": {},
   "source": [
    "# zero_grad(set_to_none=True) の効果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87cda43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecdd4995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc793df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユーティリティ関数を定義\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TIMINGS = defaultdict(list)\n",
    "\n",
    "@contextmanager\n",
    "def timed(label: str, sync_cuda: bool = True, record: bool = True, echo: bool = True):\n",
    "    \"\"\"\n",
    "    時間を計測するためのユーティリティ関数\n",
    "    使い方としては、以下のように使う。\n",
    "    ```\n",
    "    with timed(\"表示したい文字列\"):\n",
    "        func()\n",
    "    ```\n",
    "    record=True のとき、経過時間（ms）を TIMINGS[label] に保存します。\n",
    "    echo=True のとき、逐次 print も行います。\n",
    "    \"\"\"\n",
    "    # ======\n",
    "    # with句に入った瞬間の処理\n",
    "    if sync_cuda and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # ======\n",
    "    # withブロック内の処理を実行する\n",
    "    yield\n",
    "\n",
    "    # ======\n",
    "    # with句から抜ける直前の処理\n",
    "    if sync_cuda and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    elapsed_ms = (t1 - t0) * 1000\n",
    "    if echo:\n",
    "        print(f\"{label}: {elapsed_ms:.2f} ms\")\n",
    "    if record:\n",
    "        TIMINGS[label].append(elapsed_ms)\n",
    "\n",
    "\n",
    "def reset_timings():\n",
    "    \"\"\"保存した全ての計測値をリセットする。\"\"\"\n",
    "    TIMINGS.clear()\n",
    "\n",
    "\n",
    "def timing_summary(labels: list[str] | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    保存された計測の要約統計量を返す。\n",
    "    - labels が指定された場合、そのラベルのみ集計。\n",
    "    - 戻り値は {label: {count, mean_ms, std_ms, min_ms, max_ms}}。\n",
    "    \"\"\"\n",
    "    items = TIMINGS.items()\n",
    "    if labels is not None:\n",
    "        items = [(k, TIMINGS[k]) for k in labels if k in TIMINGS]\n",
    "\n",
    "    summary = {}\n",
    "    for k, vals in items:\n",
    "        arr = np.asarray(vals, dtype=np.float64)\n",
    "        n = int(arr.size)\n",
    "        mean = float(arr.mean()) if n > 0 else 0.0\n",
    "        std = float(arr.std(ddof=1)) if n > 1 else 0.0\n",
    "        vmin = float(arr.min()) if n > 0 else 0.0\n",
    "        vmax = float(arr.max()) if n > 0 else 0.0\n",
    "        summary[k] = {\n",
    "            \"count\": n,\n",
    "            \"mean_ms\": mean,\n",
    "            \"std_ms\": std,\n",
    "            \"min_ms\": vmin,\n",
    "            \"max_ms\": vmax,\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "\n",
    "def print_timing_summary(labels: list[str] | None = None) -> None:\n",
    "    \"\"\"保存された計測の要約統計量を読みやすく表示する。\"\"\"\n",
    "    summary = timing_summary(labels)\n",
    "    if not summary:\n",
    "        print(\"No timings recorded.\")\n",
    "        return\n",
    "    for k, v in summary.items():\n",
    "        print(\n",
    "            f\"{k}: n={v['count']}, mean={v['mean_ms']:.2f} ms, \"\n",
    "            f\"std={v['std_ms']:.2f} ms, min={v['min_ms']:.2f} ms, max={v['max_ms']:.2f} ms\"\n",
    "        )\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccbc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパラの設定\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc08e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10のクラスラベル\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "def setup_transform() -> tuple[transforms.Compose, transforms.Compose]:\n",
    "    \"\"\"\n",
    "    CIFAR-10 のデータセットの前処理を定義する。\n",
    "    \"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    return train_transform, test_transform\n",
    "\n",
    "\n",
    "def setup_dataset(dataset_path:str=\"./data\")->tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    CIFAR-10 のデータセットをダウンロードし、DataLoader を返す。\n",
    "    \"\"\"\n",
    "    train_transform, test_transform = setup_transform()\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=dataset_path, train=True, download=True, transform=train_transform\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=dataset_path, train=False, download=True, transform=test_transform\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def setup_dataloader(is_pin: bool = False, dataset_path:str=\"./data\") -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    CIFAR-10 のデータセットを DataLoader として返す。\n",
    "    \"\"\"\n",
    "    train_dataset, test_dataset = setup_dataset(dataset_path)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        pin_memory=is_pin\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        pin_memory=is_pin\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def setup_elements():\n",
    "    model = torchvision.models.resnet50(weights=None)\n",
    "\n",
    "    # CIFAR-10は画像サイズが32x32と小さいため、最初の畳み込み層とプーリング層を調整\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity() # MaxPool層を無効化\n",
    "\n",
    "    # ResNet-50の最終層（全結合層）をCIFAR-10の10クラス分類用に変更\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    # 学習率をスケジューリング（例：10エポック毎に学習率を0.1倍にする）\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    return model, criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMetric:\n",
    "    def __init__(self):\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, batch_avg: float, n: int = 1):\n",
    "        self.sum += batch_avg * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def avg(self) -> float:\n",
    "        return self.sum / self.count if self.count > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_1epoch(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_loader,\n",
    "):\n",
    "    metrics = {\n",
    "        \"loss\": AverageMetric(),\n",
    "        \"acc\": AverageMetric(),\n",
    "    }\n",
    "    model.train()\n",
    "    for idx, (Xs, ys) in enumerate(train_loader):\n",
    "        Xs, ys = Xs.to(DEVICE), ys.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(Xs)\n",
    "        loss = criterion(outputs, ys)\n",
    "        loss.backward()\n",
    "\n",
    "        if hasattr(optimizer, \"step\"):\n",
    "            optimizer.step()\n",
    "        if hasattr(scheduler, \"step\"):\n",
    "            scheduler.step()\n",
    "\n",
    "        # Update metrics\n",
    "        num_samples = ys.size(0)\n",
    "        metrics[\"loss\"].update(loss.item(), n=num_samples)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        metrics[\"acc\"].update((preds == ys).float().mean().item(), n=num_samples)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_1epoch_with_nograd(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_loader,\n",
    "):\n",
    "    \"\"\"zero_grad()で、オプション`set_to_none=True`を使用して勾配をNoneにする\"\"\"\n",
    "    metrics = {\n",
    "        \"loss\": AverageMetric(),\n",
    "        \"acc\": AverageMetric(),\n",
    "    }\n",
    "    model.train()\n",
    "    for idx, (Xs, ys) in enumerate(train_loader):\n",
    "        Xs, ys = Xs.to(DEVICE), ys.to(DEVICE)\n",
    "        optimizer.zero_grad(set_to_none=True) # ここで勾配をゼロにする.\n",
    "        outputs = model(Xs)\n",
    "        loss = criterion(outputs, ys)\n",
    "        loss.backward()\n",
    "\n",
    "        if hasattr(optimizer, \"step\"):\n",
    "            optimizer.step()\n",
    "        if hasattr(scheduler, \"step\"):\n",
    "            scheduler.step()\n",
    "\n",
    "        # Update metrics\n",
    "        num_samples = ys.size(0)\n",
    "        metrics[\"loss\"].update(loss.item(), n=num_samples)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        metrics[\"acc\"].update((preds == ys).float().mean().item(), n=num_samples)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def eval(\n",
    "    model,\n",
    "    criterion,\n",
    "    test_loader,\n",
    "):\n",
    "    model.eval()\n",
    "    metrics = {\n",
    "        \"loss\": AverageMetric(),\n",
    "        \"acc\": AverageMetric(),\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        for idx, (Xs, ys) in enumerate(test_loader):\n",
    "            Xs, ys = Xs.to(DEVICE), ys.to(DEVICE)\n",
    "            outputs = model(Xs)\n",
    "            loss = criterion(outputs, ys)\n",
    "\n",
    "            # Update metrics\n",
    "            num_samples = ys.size(0)\n",
    "            metrics[\"loss\"].update(loss.item(), n=num_samples)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            metrics[\"acc\"].update((preds == ys).float().mean().item(), n=num_samples)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6018f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noraml_training: 78488.78 ms\n",
      "\n",
      "noraml_training: 76510.33 ms\n",
      "\n",
      "noraml_training: 77398.08 ms\n",
      "\n",
      "noraml_training: 76064.51 ms\n",
      "\n",
      "noraml_training: 75450.02 ms\n",
      "\n",
      "loss: 2.965869669342041, acc: 0.1271\n",
      "noraml_training: n=5, mean=76782.34 ms, std=1189.06 ms, min=75450.02 ms, max=78488.78 ms\n"
     ]
    }
   ],
   "source": [
    "# 通常の学習条件で実行速度を計測する\n",
    "reset_timings()\n",
    "\n",
    "train_loader, test_loader = setup_dataloader(is_pin=False)\n",
    "model, criterion, optimizer, scheduler = setup_elements()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    with timed(\"noraml_training\", echo=True):\n",
    "        train_1epoch(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            train_loader,\n",
    "        )\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "metrics = eval(\n",
    "    model,\n",
    "    criterion,\n",
    "    test_loader,\n",
    ")\n",
    "print(f\"loss: {metrics['loss'].avg}, acc: {metrics['acc'].avg}\")\n",
    "\n",
    "# 要約を表示\n",
    "print_timing_summary([\"noraml_training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d725d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_to_none=True: 75386.75 ms\n",
      "\n",
      "set_to_none=True: 76258.83 ms\n",
      "\n",
      "set_to_none=True: 75747.22 ms\n",
      "\n",
      "set_to_none=True: 75894.83 ms\n",
      "\n",
      "set_to_none=True: 75559.26 ms\n",
      "\n",
      "loss: 3.168150677108765, acc: 0.1341\n",
      "set_to_none=True: n=5, mean=75769.38 ms, std=334.05 ms, min=75386.75 ms, max=76258.83 ms\n"
     ]
    }
   ],
   "source": [
    "# オプション`set_to_none=True`条件で実行速度を計測する\n",
    "reset_timings()\n",
    "\n",
    "train_loader, test_loader = setup_dataloader(is_pin=False)\n",
    "model, criterion, optimizer, scheduler = setup_elements()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    with timed(\"set_to_none=True\", echo=True):\n",
    "        train_1epoch_with_nograd(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            train_loader,\n",
    "        )\n",
    "    print()\n",
    "\n",
    "\n",
    "metrics = eval(\n",
    "    model,\n",
    "    criterion,\n",
    "    test_loader,\n",
    ")\n",
    "print(f\"loss: {metrics['loss'].avg}, acc: {metrics['acc'].avg}\")\n",
    "\n",
    "# 要約を表示\n",
    "print_timing_summary([\"set_to_none=True\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f1715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
