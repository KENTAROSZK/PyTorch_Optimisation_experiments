{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99cf793",
   "metadata": {},
   "source": [
    "# hugging faceモデルでの`torch.compile`の効果\n",
    "\n",
    "- hugging faceモデルでもtorch.compileが使えるみたいなので、実行速度を計測してみる\n",
    "- [利用するモデル：SakanaAI/TinySwallow-1.5B](https://huggingface.co/SakanaAI/TinySwallow-1.5B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc5f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.suzuki/pytorch_optimisation/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1b0b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:4\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47175778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユーティリティ関数を定義\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TIMINGS = defaultdict(list)\n",
    "\n",
    "@contextmanager\n",
    "def timed(label: str, sync_cuda: bool = True, record: bool = True, echo: bool = True):\n",
    "    \"\"\"\n",
    "    時間を計測するためのユーティリティ関数\n",
    "    使い方としては、以下のように使う。\n",
    "    ```\n",
    "    with timed(\"表示したい文字列\"):\n",
    "        func()\n",
    "    ```\n",
    "    record=True のとき、経過時間（ms）を TIMINGS[label] に保存します。\n",
    "    echo=True のとき、逐次 print も行います。\n",
    "    \"\"\"\n",
    "    # ======\n",
    "    # with句に入った瞬間の処理\n",
    "    if sync_cuda and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # ======\n",
    "    # withブロック内の処理を実行する\n",
    "    yield\n",
    "\n",
    "    # ======\n",
    "    # with句から抜ける直前の処理\n",
    "    if sync_cuda and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    elapsed_ms = (t1 - t0) * 1000\n",
    "    if echo:\n",
    "        print(f\"{label}: {elapsed_ms:.2f} ms\")\n",
    "    if record:\n",
    "        TIMINGS[label].append(elapsed_ms)\n",
    "\n",
    "\n",
    "def reset_timings():\n",
    "    \"\"\"保存した全ての計測値をリセットする。\"\"\"\n",
    "    TIMINGS.clear()\n",
    "\n",
    "\n",
    "def timing_summary(labels: list[str] | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    保存された計測の要約統計量を返す。\n",
    "    - labels が指定された場合、そのラベルのみ集計。\n",
    "    - 戻り値は {label: {count, mean_ms, std_ms, min_ms, max_ms}}。\n",
    "    \"\"\"\n",
    "    items = TIMINGS.items()\n",
    "    if labels is not None:\n",
    "        items = [(k, TIMINGS[k]) for k in labels if k in TIMINGS]\n",
    "\n",
    "    summary = {}\n",
    "    for k, vals in items:\n",
    "        arr = np.asarray(vals, dtype=np.float64)\n",
    "        n = int(arr.size)\n",
    "        mean = float(arr.mean()) if n > 0 else 0.0\n",
    "        std = float(arr.std(ddof=1)) if n > 1 else 0.0\n",
    "        vmin = float(arr.min()) if n > 0 else 0.0\n",
    "        vmax = float(arr.max()) if n > 0 else 0.0\n",
    "        summary[k] = {\n",
    "            \"count\": n,\n",
    "            \"mean_ms\": mean,\n",
    "            \"std_ms\": std,\n",
    "            \"min_ms\": vmin,\n",
    "            \"max_ms\": vmax,\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "\n",
    "def print_timing_summary(labels: list[str] | None = None) -> None:\n",
    "    \"\"\"保存された計測の要約統計量を読みやすく表示する。\"\"\"\n",
    "    summary = timing_summary(labels)\n",
    "    if not summary:\n",
    "        print(\"No timings recorded.\")\n",
    "        return\n",
    "    for k, v in summary.items():\n",
    "        print(\n",
    "            f\"{k}: n={v['count']}, mean={v['mean_ms']:.2f} ms, \"\n",
    "            f\"std={v['std_ms']:.2f} ms, min={v['min_ms']:.2f} ms, max={v['max_ms']:.2f} ms\"\n",
    "        )\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f42d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SakanaAI/TinySwallow-1.5B\n",
    "model_name = \"SakanaAI/TinySwallow-1.5B\"\n",
    "cache_directory = \"./model_cache\"\n",
    "\n",
    "\n",
    "def load_elements(is_compile:bool=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_directory\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=cache_directory\n",
    "    )\n",
    "\n",
    "    if is_compile:\n",
    "        try:\n",
    "            model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during torch.compile: {e}\")\n",
    "                print(\"Falling back to eager execution.\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf25609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 入力テンソルをモデルのデバイスへ移動（CPU/ CUDA の不一致による警告回避）\n",
    "    try:\n",
    "        model_device = getattr(model, 'device', None)\n",
    "        if model_device is None:\n",
    "            # フォールバック（単一デバイス想定）\n",
    "            model_device = DEVICE\n",
    "        inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "    except Exception:\n",
    "        # 万一 device を取得できない/複数デバイスに分散の場合は、定義済み DEVICE を利用\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    decoded = tokenizer.decode(outputs[0])\n",
    "    return decoded, input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f842d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_compile=False: 2466.67 ms\n",
      "\n",
      "\n",
      "created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Assistant! How can I help you today?<|im_end|>\n",
      "is_compile=False: n=1, mean=2466.67 ms, std=0.00 ms, min=2466.67 ms, max=2466.67 ms\n"
     ]
    }
   ],
   "source": [
    "# compileなしで実行速度を計測\n",
    "reset_timings()\n",
    "is_compile=False\n",
    "model, tokenizer = load_elements(is_compile=is_compile)\n",
    "with timed(f\"{is_compile=}\"):\n",
    "    decoded, input_len = generate_response(model, tokenizer)\n",
    "print(\"\\n\")\n",
    "print(f\"{decoded[input_len:]}\")\n",
    "\n",
    "# 要約を表示\n",
    "print_timing_summary([f\"{is_compile=}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bad73a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_compile=True: 818.20 ms\n",
      "\n",
      "\n",
      "created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Who are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hello! How can I help you today?<|im_end|>\n",
      "is_compile=True: n=1, mean=818.20 ms, std=0.00 ms, min=818.20 ms, max=818.20 ms\n"
     ]
    }
   ],
   "source": [
    "# compileありで実行速度を計測\n",
    "reset_timings()\n",
    "is_compile=True\n",
    "model, tokenizer = load_elements(is_compile=is_compile)\n",
    "with timed(f\"{is_compile=}\"):\n",
    "    decoded, input_len = generate_response(model, tokenizer)\n",
    "print(\"\\n\")\n",
    "print(f\"{decoded[input_len:]}\")\n",
    "\n",
    "# 要約を表示\n",
    "print_timing_summary([f\"{is_compile=}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e2364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
